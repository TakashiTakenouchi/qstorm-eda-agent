#!/usr/bin/env python3
"""
Q-Storm EDA Distribution Analyzer - Hybrid Streamlit App
=========================================================

ç¢ºç‡åˆ†å¸ƒåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆï¼‰
- ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠå¼UIï¼ˆãƒ¡ã‚¤ãƒ³ï¼‰
- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹NLPï¼ˆè‡ªç„¶è¨€èªå…¥åŠ›ã‚µãƒãƒ¼ãƒˆï¼‰
- APIã‚­ãƒ¼ä¸è¦ã§å‹•ä½œ

ä½¿ç”¨æ–¹æ³•:
    streamlit run streamlit_app.py
"""

import os
import re
from pathlib import Path

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import streamlit as st
from scipy import stats

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
plt.rcParams['font.family'] = ['DejaVu Sans', 'Hiragino Sans', 'Yu Gothic', 'Meiryo', 'sans-serif']


# =============================================================================
# å®šæ•°å®šç¾©
# =============================================================================

DEFAULT_DATA_PATH = r"C:\Users\ç«¹ä¹‹å†…éš†\Documents\MBS_Lessons\MBS2025\Data Set\Ensuring consistency between tabular data and time series forecast data\fixed_extended_store_data_2024-FIX_kaizen_monthlyvol6_new.xlsx"

TARGET_COLUMNS = [
    "Total_Sales", "gross_profit", "discount", "purchasing", "rent",
    "personnel_expenses", "depreciation", "sales_promotion",
    "head_office_expenses", "operating_cost", "Operating_profit",
    "Mens_JACKETS&OUTER2", "Mens_KNIT", "Mens_PANTS",
    "WOMEN'S_JACKETS2", "WOMEN'S_TOPS", "WOMEN'S_ONEPIECE",
    "WOMEN'S_bottoms", "WOMEN'S_SCARF & STOLES",
    "Inventory", "Months_of_inventory", "BEP",
    "Average_Temperature", "Number_of_guests", "Price_per_customer",
]

COLUMN_DEFINITIONS = {
    "Total_Sales": "å£²ä¸Šé«˜",
    "gross_profit": "ç²—åˆ©",
    "discount": "å€¤å¼•ã",
    "purchasing": "ä»•å…¥",
    "rent": "å®¶è³ƒ",
    "personnel_expenses": "äººä»¶è²»",
    "depreciation": "æ¸›ä¾¡å„Ÿå´è²»ç”¨",
    "sales_promotion": "è²©å£²ä¿ƒé€²è²»",
    "head_office_expenses": "æœ¬ç¤¾è²»ç”¨",
    "operating_cost": "æ¥­å‹™è²»ç”¨",
    "Operating_profit": "å–¶æ¥­åˆ©ç›Š",
    "Mens_JACKETS&OUTER2": "ç”·æ€§ç”¨JACKETS&OUTERå£²ä¸Šé«˜",
    "Mens_KNIT": "ç”·æ€§ç”¨KNITå£²ä¸Šé«˜",
    "Mens_PANTS": "ç”·æ€§ç”¨PANTSå£²ä¸Šé«˜",
    "WOMEN'S_JACKETS2": "å¥³æ€§ç”¨JACKETSå£²ä¸Šé«˜",
    "WOMEN'S_TOPS": "å¥³æ€§ç”¨TOPSå£²ä¸Šé«˜",
    "WOMEN'S_ONEPIECE": "å¥³æ€§ç”¨ONEPIECEå£²ä¸Šé«˜",
    "WOMEN'S_bottoms": "å¥³æ€§ç”¨bottomså£²ä¸Šé«˜",
    "WOMEN'S_SCARF & STOLES": "å¥³æ€§ç”¨SCARF&STOLESå£²ä¸Šé«˜",
    "Inventory": "åœ¨åº«é‡‘é¡",
    "Months_of_inventory": "åœ¨åº«æœˆæ•°",
    "BEP": "æç›Šåˆ†å²ç‚¹",
    "Average_Temperature": "å¹³å‡æ°—æ¸©",
    "Number_of_guests": "å®¢æ•°",
    "Price_per_customer": "å®¢å˜ä¾¡",
}

# é€†å¼•ãè¾æ›¸ï¼ˆæ—¥æœ¬èªâ†’è‹±èªï¼‰
COLUMN_REVERSE = {v: k for k, v in COLUMN_DEFINITIONS.items()}


# =============================================================================
# ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹NLP
# =============================================================================

class RuleBasedNLP:
    """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒãƒ³ã‚°ã«ã‚ˆã‚‹è‡ªç„¶è¨€èªè§£æ"""

    # æ„å›³ãƒ‘ã‚¿ãƒ¼ãƒ³
    INTENT_PATTERNS = {
        "histogram": ["ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ", "åˆ†å¸ƒ", "ã°ã‚‰ã¤ã", "æ•£ã‚‰ã°ã‚Š", "ã‚°ãƒ©ãƒ•", "å¯è¦–åŒ–"],
        "analyze": ["åˆ†æ", "åˆ¤å®š", "åˆ¤åˆ¥", "ç‰¹å®š", "èª¿ã¹", "ç¢ºèª"],
        "compare": ["æ¯”è¼ƒ", "é•ã„", "å·®", "å¯¾æ¯”", "vs", "ï¼¶ï¼³"],
        "normality": ["æ­£è¦", "æ­£è¦æ€§", "ã‚¬ã‚¦ã‚¹", "æ¤œå®š", "ãƒ†ã‚¹ãƒˆ"],
        "summary": ["æ¦‚è¦", "ã‚µãƒãƒªãƒ¼", "ä¸€è¦§", "ã¾ã¨ã‚", "å…¨ä½“"],
    }

    # åº—èˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³
    SHOP_PATTERNS = {
        "æµæ¯”å¯¿": ["æµæ¯”å¯¿", "ãˆã³ã™", "ã‚¨ãƒ“ã‚¹", "ebisu"],
        "æ¨ªæµœå…ƒç”º": ["æ¨ªæµœ", "å…ƒç”º", "ã‚ˆã“ã¯ã¾", "ãƒ¨ã‚³ãƒãƒ", "yokohama"],
    }

    @classmethod
    def parse_query(cls, query: str) -> dict:
        """è‡ªç„¶è¨€èªã‚¯ã‚¨ãƒªã‚’è§£æ"""
        query_lower = query.lower()
        result = {
            "intent": None,
            "column": None,
            "shop": None,
            "raw_query": query,
            "confidence": 0.0,
        }

        # æ„å›³æ¤œå‡º
        for intent, patterns in cls.INTENT_PATTERNS.items():
            for pattern in patterns:
                if pattern in query:
                    result["intent"] = intent
                    result["confidence"] += 0.3
                    break
            if result["intent"]:
                break

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ„å›³
        if not result["intent"]:
            result["intent"] = "analyze"
            result["confidence"] += 0.1

        # ã‚«ãƒ©ãƒ æ¤œå‡ºï¼ˆæ—¥æœ¬èªåï¼‰
        for ja_name, en_name in COLUMN_REVERSE.items():
            if ja_name in query:
                result["column"] = en_name
                result["confidence"] += 0.4
                break

        # ã‚«ãƒ©ãƒ æ¤œå‡ºï¼ˆè‹±èªåï¼‰
        if not result["column"]:
            for col in TARGET_COLUMNS:
                if col.lower() in query_lower:
                    result["column"] = col
                    result["confidence"] += 0.4
                    break

        # ã‚«ãƒ©ãƒ æ¤œå‡ºï¼ˆéƒ¨åˆ†ä¸€è‡´ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        if not result["column"]:
            keyword_map = {
                "å£²ä¸Š": "Total_Sales",
                "å®¢æ•°": "Number_of_guests",
                "å®¢å˜ä¾¡": "Price_per_customer",
                "åˆ©ç›Š": "Operating_profit",
                "ç²—åˆ©": "gross_profit",
                "åœ¨åº«": "Inventory",
                "äººä»¶è²»": "personnel_expenses",
                "å®¶è³ƒ": "rent",
            }
            for keyword, col in keyword_map.items():
                if keyword in query:
                    result["column"] = col
                    result["confidence"] += 0.3
                    break

        # åº—èˆ—æ¤œå‡º
        for shop, patterns in cls.SHOP_PATTERNS.items():
            for pattern in patterns:
                if pattern in query or pattern in query_lower:
                    result["shop"] = shop
                    result["confidence"] += 0.2
                    break
            if result["shop"]:
                break

        # æ¯”è¼ƒæ„å›³ã§ä¸¡åº—èˆ—æ¤œå‡º
        if result["intent"] == "compare":
            result["shop"] = None  # æ¯”è¼ƒæ™‚ã¯ä¸¡åº—èˆ—ä½¿ç”¨

        result["confidence"] = min(result["confidence"], 1.0)
        return result

    @classmethod
    def get_suggestion(cls, parsed: dict) -> str:
        """è§£æçµæœã‹ã‚‰ææ¡ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ"""
        intent = parsed["intent"]
        column = parsed["column"]
        shop = parsed["shop"]

        intent_names = {
            "histogram": "ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ è¡¨ç¤º",
            "analyze": "åˆ†å¸ƒåˆ†æ",
            "compare": "åº—èˆ—é–“æ¯”è¼ƒ",
            "normality": "æ­£è¦æ€§æ¤œå®š",
            "summary": "å…¨ä½“æ¦‚è¦",
        }

        msg = f"ğŸ“ è§£é‡ˆçµæœ: **{intent_names.get(intent, intent)}**"
        if column:
            msg += f" / ã‚«ãƒ©ãƒ : **{COLUMN_DEFINITIONS.get(column, column)}**"
        if shop:
            msg += f" / åº—èˆ—: **{shop}**"
        msg += f" (ä¿¡é ¼åº¦: {parsed['confidence']*100:.0f}%)"

        return msg


# =============================================================================
# çµ±è¨ˆåˆ†æé–¢æ•°
# =============================================================================

def analyze_distribution(data: np.ndarray, column_name: str) -> dict:
    """ç¢ºç‡åˆ†å¸ƒã‚’åˆ†æ"""
    n = len(data)
    mean_val = float(np.mean(data))
    variance = float(np.var(data, ddof=1))
    std_dev = float(np.std(data, ddof=1))
    skewness = float(stats.skew(data))
    kurtosis = float(stats.kurtosis(data))

    dispersion_index = variance / mean_val if mean_val > 0 else float('inf')
    is_discrete = all(float(x).is_integer() for x in data) and data.min() >= 0

    # æ­£è¦æ€§æ¤œå®š
    sample_data = data if len(data) <= 5000 else np.random.choice(data, size=5000, replace=False)
    shapiro_stat, shapiro_p = stats.shapiro(sample_data)

    data_standardized = (data - mean_val) / std_dev if std_dev > 0 else data
    ks_stat, ks_p = stats.kstest(data_standardized, 'norm')
    normality_passed = shapiro_p > 0.05 and ks_p > 0.05

    # åˆ†å¸ƒãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°
    fits = {}

    # æ­£è¦åˆ†å¸ƒ
    mu, sigma = stats.norm.fit(data)
    ll_normal = np.sum(stats.norm.logpdf(data, mu, sigma))
    aic_normal = 4 - 2 * ll_normal
    fits["normal"] = {"parameters": {"mu": round(mu, 2), "sigma": round(sigma, 2)}, "aic": round(aic_normal, 2)}

    # ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ
    if mean_val > 0 and data.min() >= 0:
        lambda_poisson = mean_val
        data_int = np.maximum(np.round(data).astype(int), 0)
        ll_poisson = np.sum(stats.poisson.logpmf(data_int, lambda_poisson))
        aic_poisson = 2 - 2 * ll_poisson
        fits["poisson"] = {"parameters": {"lambda": round(lambda_poisson, 2)}, "aic": round(aic_poisson, 2)}

    # è² ã®äºŒé …åˆ†å¸ƒ
    if mean_val > 0 and variance > mean_val and data.min() >= 0:
        p = mean_val / variance if variance > 0 else 0.5
        p = max(0.001, min(0.999, p))
        r = mean_val * p / (1 - p) if p < 1 else 1.0
        r = max(0.1, r)
        data_int = np.maximum(np.round(data).astype(int), 0)
        ll_nbinom = np.sum(stats.nbinom.logpmf(data_int, r, p))
        aic_nbinom = 4 - 2 * ll_nbinom
        fits["negative_binomial"] = {"parameters": {"r": round(r, 2), "p": round(p, 4)}, "aic": round(aic_nbinom, 2)}

    valid_fits = {k: v for k, v in fits.items() if np.isfinite(v["aic"])}
    best_by_aic = min(valid_fits.items(), key=lambda x: x[1]["aic"])[0] if valid_fits else "unknown"

    # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
    evidence = []
    confidence = 0.0

    if not is_discrete:
        distribution_type = "normal"
        evidence.append("ãƒ‡ãƒ¼ã‚¿ãŒé€£ç¶šå€¤ã§ã‚ã‚‹")
        confidence = 0.7
        if normality_passed:
            evidence.append("æ­£è¦æ€§æ¤œå®šã«ãƒ‘ã‚¹ã—ãŸ")
            confidence += 0.2
        if best_by_aic == "normal":
            evidence.append("AICã§æ­£è¦åˆ†å¸ƒãŒæœ€è‰¯")
            confidence += 0.1
    else:
        if 0.8 <= dispersion_index <= 1.2:
            distribution_type = "poisson"
            evidence.append(f"åˆ†æ•£/å¹³å‡æ¯” = {dispersion_index:.2f} â‰ˆ 1.0")
            confidence = 0.75
        elif dispersion_index > 1.2:
            distribution_type = "negative_binomial"
            evidence.append(f"åˆ†æ•£/å¹³å‡æ¯” = {dispersion_index:.2f} > 1.0ï¼ˆéåˆ†æ•£ï¼‰")
            confidence = 0.75
        else:
            distribution_type = "poisson"
            evidence.append(f"åˆ†æ•£/å¹³å‡æ¯” = {dispersion_index:.2f}")
            confidence = 0.5

    return {
        "column": column_name,
        "column_ja": COLUMN_DEFINITIONS.get(column_name, column_name),
        "distribution_type": distribution_type,
        "confidence": round(min(confidence, 1.0), 2),
        "statistics": {
            "n_samples": n,
            "mean": round(mean_val, 2),
            "variance": round(variance, 2),
            "std_dev": round(std_dev, 2),
            "skewness": round(skewness, 4),
            "kurtosis": round(kurtosis, 4),
            "dispersion_index": round(dispersion_index, 4)
        },
        "normality_tests": {
            "shapiro_wilk": {"statistic": round(shapiro_stat, 4), "p_value": round(shapiro_p, 4)},
            "kolmogorov_smirnov": {"statistic": round(ks_stat, 4), "p_value": round(ks_p, 4)},
            "passed": normality_passed
        },
        "distribution_fits": fits,
        "best_fit_by_aic": best_by_aic,
        "evidence": evidence,
        "is_discrete": is_discrete
    }


def compare_shops(df: pd.DataFrame, column: str) -> dict:
    """åº—èˆ—é–“ã®åˆ†å¸ƒã‚’æ¯”è¼ƒ"""
    shops = df["shop"].unique()
    comparison = {}

    for shop in shops:
        shop_data = df[df["shop"] == shop][column].dropna()
        if len(shop_data) > 0:
            comparison[shop] = {
                "n_samples": len(shop_data),
                "mean": round(float(shop_data.mean()), 2),
                "std": round(float(shop_data.std()), 2),
                "min": round(float(shop_data.min()), 2),
                "max": round(float(shop_data.max()), 2),
                "median": round(float(shop_data.median()), 2)
            }

    test_result = None
    if len(shops) == 2:
        data1 = df[df["shop"] == shops[0]][column].dropna()
        data2 = df[df["shop"] == shops[1]][column].dropna()
        if len(data1) > 0 and len(data2) > 0:
            t_stat, t_p = stats.ttest_ind(data1, data2)
            u_stat, u_p = stats.mannwhitneyu(data1, data2, alternative='two-sided')
            test_result = {
                "t_test": {"statistic": round(t_stat, 4), "p_value": round(t_p, 4)},
                "mann_whitney_u": {"statistic": round(u_stat, 4), "p_value": round(u_p, 4)},
                "significant_difference": t_p < 0.05 or u_p < 0.05
            }

    return {
        "column": column,
        "column_ja": COLUMN_DEFINITIONS.get(column, column),
        "shops_comparison": comparison,
        "statistical_test": test_result
    }


def create_histogram_figure(data: np.ndarray, column_name: str, shop_filter: str = None) -> plt.Figure:
    """ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ä½œæˆ"""
    fig, ax = plt.subplots(figsize=(10, 6))
    mean_val = float(data.mean())
    std_val = float(data.std())

    ax.hist(data, bins=15, edgecolor='black', alpha=0.7, color='#667eea')
    ax.set_xlabel(COLUMN_DEFINITIONS.get(column_name, column_name), fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)

    title = f'{COLUMN_DEFINITIONS.get(column_name, column_name)}'
    if shop_filter:
        title += f' ({shop_filter})'
    ax.set_title(title, fontsize=14)

    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:,.0f}')
    ax.axvline(mean_val - std_val, color='orange', linestyle=':', alpha=0.7, label=f'-1Ïƒ: {mean_val-std_val:,.0f}')
    ax.axvline(mean_val + std_val, color='orange', linestyle=':', alpha=0.7, label=f'+1Ïƒ: {mean_val+std_val:,.0f}')
    ax.legend()

    plt.tight_layout()
    return fig


def get_distribution_explanation(result: dict) -> str:
    """åˆ†å¸ƒåˆ¤å®šçµæœã®è§£èª¬"""
    dist_type = result["distribution_type"]
    stats_info = result["statistics"]
    evidence = result["evidence"]

    dist_names = {
        "normal": "æ­£è¦åˆ†å¸ƒ (Normal Distribution)",
        "poisson": "ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒ (Poisson Distribution)",
        "negative_binomial": "è² ã®äºŒé …åˆ†å¸ƒ (Negative Binomial Distribution)"
    }

    explanation = f"""
### ğŸ¯ åˆ†å¸ƒåˆ¤å®šçµæœ: {dist_names.get(dist_type, dist_type)}

**ä¿¡é ¼åº¦**: {result['confidence'] * 100:.0f}%

#### åˆ¤å®šæ ¹æ‹ 
"""
    for e in evidence:
        explanation += f"- {e}\n"

    explanation += f"""
#### ğŸ“Š åŸºæœ¬çµ±è¨ˆé‡
| çµ±è¨ˆé‡ | å€¤ |
|--------|-----|
| ã‚µãƒ³ãƒ—ãƒ«æ•° | {stats_info['n_samples']:,} |
| å¹³å‡ | {stats_info['mean']:,.2f} |
| æ¨™æº–åå·® | {stats_info['std_dev']:,.2f} |
| åˆ†æ•£ | {stats_info['variance']:,.2f} |
| æ­ªåº¦ | {stats_info['skewness']:.4f} |
| å°–åº¦ | {stats_info['kurtosis']:.4f} |
| åˆ†æ•£/å¹³å‡æ¯” | {stats_info['dispersion_index']:.4f} |

#### ğŸ§ª æ­£è¦æ€§æ¤œå®š
| æ¤œå®š | çµ±è¨ˆé‡ | på€¤ | çµæœ |
|------|--------|-----|------|
| Shapiro-Wilk | {result['normality_tests']['shapiro_wilk']['statistic']:.4f} | {result['normality_tests']['shapiro_wilk']['p_value']:.4f} | {'âœ… Pass' if result['normality_tests']['shapiro_wilk']['p_value'] > 0.05 else 'âŒ Fail'} |
| Kolmogorov-Smirnov | {result['normality_tests']['kolmogorov_smirnov']['statistic']:.4f} | {result['normality_tests']['kolmogorov_smirnov']['p_value']:.4f} | {'âœ… Pass' if result['normality_tests']['kolmogorov_smirnov']['p_value'] > 0.05 else 'âŒ Fail'} |
"""

    # ãƒ“ã‚¸ãƒã‚¹è§£é‡ˆ
    explanation += "\n#### ğŸ’¼ ãƒ“ã‚¸ãƒã‚¹çš„è§£é‡ˆ\n"
    if dist_type == "normal":
        explanation += """
æ­£è¦åˆ†å¸ƒã«å¾“ã†ãƒ‡ãƒ¼ã‚¿ã¯ã€å¹³å‡å€¤ã‚’ä¸­å¿ƒã«å¯¾ç§°çš„ã«åˆ†å¸ƒã—ã¦ã„ã¾ã™ã€‚
å¤šãã®ç‹¬ç«‹ã—ãŸè¦å› ãŒåŠ æ³•çš„ã«å½±éŸ¿ã—ã¦ã„ã‚‹å ´åˆã«è¦‹ã‚‰ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚
- **ç®¡ç†æŒ‡æ¨™**: å¹³å‡Â±2Ïƒï¼ˆç´„95%ï¼‰ã®ç¯„å›²ã§ç®¡ç†
- **ç•°å¸¸æ¤œçŸ¥**: 3Ïƒã‚’è¶…ãˆã‚‹å€¤ã¯ç•°å¸¸å€¤ã®å¯èƒ½æ€§
"""
    elif dist_type == "poisson":
        explanation += """
ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã¯ã€ä¸€å®šæœŸé–“ã«ãŠã‘ã‚‹ç™ºç”Ÿä»¶æ•°ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ã®ã«é©ã—ã¦ã„ã¾ã™ã€‚
- **ç‰¹å¾´**: åˆ†æ•£ã¨å¹³å‡ãŒã»ã¼ç­‰ã—ã„
- **é©ç”¨ä¾‹**: å®¢æ•°ã€æ¥åº—ä»¶æ•°ã€å•ã„åˆã‚ã›ä»¶æ•°
- **äºˆæ¸¬**: Î»ï¼ˆå¹³å‡ç™ºç”Ÿç‡ï¼‰ã‚’ä½¿ã£ã¦ç¢ºç‡è¨ˆç®—å¯èƒ½
"""
    elif dist_type == "negative_binomial":
        explanation += """
è² ã®äºŒé …åˆ†å¸ƒã¯ã€éåˆ†æ•£ï¼ˆåˆ†æ•£ > å¹³å‡ï¼‰ã®ã‚ã‚‹ã‚«ã‚¦ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã«é©ã—ã¦ã„ã¾ã™ã€‚
- **ç‰¹å¾´**: ãƒã‚¢ã‚½ãƒ³åˆ†å¸ƒã‚ˆã‚Šè£¾ãŒé‡ã„
- **åŸå› **: å­£ç¯€å¤‰å‹•ã€åº—èˆ—å·®ã€é¡§å®¢ã‚»ã‚°ãƒ¡ãƒ³ãƒˆå·®
- **å¯¾ç­–**: å±¤åˆ¥åˆ†æã§å¤‰å‹•è¦å› ã‚’ç‰¹å®šã™ã‚‹ã“ã¨ã‚’æ¨å¥¨
"""

    return explanation


# =============================================================================
# Streamlit UI
# =============================================================================

def main():
    st.set_page_config(
        page_title="Q-Storm EDA Distribution Analyzer",
        page_icon="ğŸ“Š",
        layout="wide"
    )

    # ã‚«ã‚¹ã‚¿ãƒ CSS
    st.markdown("""
    <style>
        .main-header {
            font-size: 2.5rem;
            font-weight: bold;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            text-align: center;
            margin-bottom: 0.5rem;
        }
        .sub-header {
            font-size: 1.1rem;
            color: #666;
            text-align: center;
            margin-bottom: 1.5rem;
        }
        .nlp-box {
            background: linear-gradient(135deg, #f5f7fa 0%, #e4e8ec 100%);
            padding: 1rem;
            border-radius: 10px;
            border-left: 4px solid #667eea;
        }
    </style>
    """, unsafe_allow_html=True)

    # ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown('<div class="main-header">ğŸ“Š Q-Storm EDA Distribution Analyzer</div>', unsafe_allow_html=True)
    st.markdown('<div class="sub-header">ç¢ºç‡åˆ†å¸ƒåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆï¼ˆAPIä¸è¦ï¼‰</div>', unsafe_allow_html=True)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    st.sidebar.header("âš™ï¸ è¨­å®š")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    uploaded_file = st.sidebar.file_uploader("ğŸ“ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['xlsx', 'xls'])

    df = None
    if uploaded_file is not None:
        try:
            df = pd.read_excel(uploaded_file)
            st.sidebar.success(f"âœ… ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œ")
        except Exception as e:
            st.sidebar.error(f"âŒ èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        if os.path.exists(DEFAULT_DATA_PATH):
            try:
                df = pd.read_excel(DEFAULT_DATA_PATH)
                st.sidebar.info(f"ğŸ“‚ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒ¼ã‚¿: {len(df):,}è¡Œ")
            except:
                pass

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    if df is None:
        st.sidebar.markdown("---")
        if st.sidebar.button("ğŸ² ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è©¦ã™"):
            np.random.seed(42)
            df = pd.DataFrame({
                "shop": np.repeat(["æµæ¯”å¯¿", "æ¨ªæµœå…ƒç”º"], 50),
                "Total_Sales": np.random.normal(5000000, 1000000, 100),
                "Number_of_guests": np.random.poisson(500, 100),
                "Operating_profit": np.random.normal(200000, 100000, 100),
                "gross_profit": np.random.normal(1500000, 300000, 100),
                "Inventory": np.random.normal(2000000, 500000, 100),
                "Price_per_customer": np.random.normal(10000, 2000, 100),
            })
            st.session_state['df'] = df
            st.rerun()

    if 'df' in st.session_state:
        df = st.session_state['df']

    if df is None:
        st.info("ğŸ‘† ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã¾ãŸã¯ã€Œã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è©¦ã™ã€ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„")
        return

    # åˆ©ç”¨å¯èƒ½ã‚«ãƒ©ãƒ 
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    available_cols = [c for c in TARGET_COLUMNS if c in numeric_cols]
    if not available_cols:
        available_cols = numeric_cols[:10]

    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    tab1, tab2, tab3 = st.tabs(["ğŸ’¬ è‡ªç„¶è¨€èªå…¥åŠ›", "ğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠ", "ğŸ“Š å…¨ä½“æ¦‚è¦"])

    # =========================
    # Tab 1: è‡ªç„¶è¨€èªå…¥åŠ›
    # =========================
    with tab1:
        st.markdown("### ğŸ’¬ è‡ªç„¶è¨€èªã§è³ªå•")
        st.markdown('<div class="nlp-box">', unsafe_allow_html=True)

        query = st.text_input(
            "è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            placeholder="ä¾‹: å£²ä¸Šé«˜ã®åˆ†å¸ƒã‚’è¦‹ãŸã„ / å®¢æ•°ã¯æ­£è¦åˆ†å¸ƒï¼Ÿ / æµæ¯”å¯¿ã¨æ¨ªæµœã‚’æ¯”è¼ƒ",
            key="nlp_query"
        )

        col1, col2 = st.columns([1, 4])
        with col1:
            analyze_btn = st.button("ğŸ” åˆ†æ", type="primary", key="nlp_analyze")

        st.markdown('</div>', unsafe_allow_html=True)

        if query and analyze_btn:
            # NLPè§£æ
            parsed = RuleBasedNLP.parse_query(query)
            st.markdown(RuleBasedNLP.get_suggestion(parsed))

            st.markdown("---")

            # è§£æçµæœã«åŸºã¥ãå‡¦ç†
            intent = parsed["intent"]
            column = parsed["column"]
            shop = parsed["shop"]

            if not column and intent != "summary":
                st.warning("âš ï¸ ã‚«ãƒ©ãƒ ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠã‚¿ãƒ–ã‚’ãŠä½¿ã„ãã ã•ã„ã€‚")
            elif intent == "compare":
                if "shop" not in df.columns:
                    st.warning("åº—èˆ—ã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    result = compare_shops(df, column)
                    st.markdown(f"### ğŸª {result['column_ja']} ã®åº—èˆ—é–“æ¯”è¼ƒ")

                    cols = st.columns(len(result['shops_comparison']))
                    for i, (shop_name, stats_data) in enumerate(result['shops_comparison'].items()):
                        with cols[i]:
                            st.metric(f"ğŸª {shop_name}", f"{stats_data['mean']:,.0f}", f"Â±{stats_data['std']:,.0f}")

                    if result['statistical_test']:
                        test = result['statistical_test']
                        if test['significant_difference']:
                            st.success(f"âœ… çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ã‚ã‚Š (tæ¤œå®š p={test['t_test']['p_value']:.4f})")
                        else:
                            st.info(f"â„¹ï¸ æœ‰æ„ãªå·®ãªã— (tæ¤œå®š p={test['t_test']['p_value']:.4f})")

            elif intent in ["histogram", "analyze", "normality"]:
                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                if shop:
                    data = df[df["shop"] == shop][column].dropna().values
                else:
                    data = df[column].dropna().values

                if len(data) > 0:
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        fig = create_histogram_figure(data, column, shop)
                        st.pyplot(fig)
                        plt.close(fig)

                    with col2:
                        result = analyze_distribution(data, column)
                        st.markdown(get_distribution_explanation(result))

            elif intent == "summary":
                st.markdown("### ğŸ“‹ å…¨ã‚«ãƒ©ãƒ æ¦‚è¦")
                summary_data = []
                for col in available_cols[:10]:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        summary_data.append({
                            "ã‚«ãƒ©ãƒ ": COLUMN_DEFINITIONS.get(col, col),
                            "å¹³å‡": f"{col_data.mean():,.0f}",
                            "æ¨™æº–åå·®": f"{col_data.std():,.0f}",
                            "æœ€å°": f"{col_data.min():,.0f}",
                            "æœ€å¤§": f"{col_data.max():,.0f}",
                        })
                st.dataframe(pd.DataFrame(summary_data), use_container_width=True)

        # è³ªå•ä¾‹
        st.markdown("---")
        st.markdown("#### ğŸ’¡ è³ªå•ä¾‹")
        example_cols = st.columns(3)
        examples = [
            "å£²ä¸Šé«˜ã®åˆ†å¸ƒã‚’è¦‹ãŸã„",
            "å®¢æ•°ã¯æ­£è¦åˆ†å¸ƒï¼Ÿ",
            "æµæ¯”å¯¿ã¨æ¨ªæµœã‚’æ¯”è¼ƒ",
            "å–¶æ¥­åˆ©ç›Šã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ",
            "åœ¨åº«ã®çµ±è¨ˆã‚’æ•™ãˆã¦",
            "å…¨ä½“ã®æ¦‚è¦ã‚’è¦‹ã›ã¦",
        ]
        for i, ex in enumerate(examples):
            with example_cols[i % 3]:
                if st.button(ex, key=f"example_{i}"):
                    st.session_state['nlp_query'] = ex
                    st.rerun()

    # =========================
    # Tab 2: ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠ
    # =========================
    with tab2:
        st.markdown("### ğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼é¸æŠå¼åˆ†æ")

        col1, col2, col3 = st.columns(3)

        with col1:
            selected_column = st.selectbox(
                "ğŸ“Š åˆ†æã‚«ãƒ©ãƒ ",
                available_cols,
                format_func=lambda x: f"{COLUMN_DEFINITIONS.get(x, x)} ({x})"
            )

        with col2:
            analysis_type = st.selectbox(
                "ğŸ”¬ åˆ†æã‚¿ã‚¤ãƒ—",
                ["ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + åˆ†å¸ƒåˆ¤å®š", "åº—èˆ—é–“æ¯”è¼ƒ", "æ­£è¦æ€§æ¤œå®šã®ã¿"]
            )

        with col3:
            shop_options = ["å…¨åº—èˆ—"]
            if "shop" in df.columns:
                shop_options += df["shop"].unique().tolist()
            selected_shop = st.selectbox("ğŸª åº—èˆ—ãƒ•ã‚£ãƒ«ã‚¿", shop_options)
            if selected_shop == "å…¨åº—èˆ—":
                selected_shop = None

        if st.button("ğŸš€ åˆ†æå®Ÿè¡Œ", type="primary", key="menu_analyze"):
            if analysis_type == "åº—èˆ—é–“æ¯”è¼ƒ":
                if "shop" not in df.columns:
                    st.warning("åº—èˆ—ã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
                else:
                    result = compare_shops(df, selected_column)
                    st.markdown(f"### ğŸª {result['column_ja']} ã®åº—èˆ—é–“æ¯”è¼ƒ")

                    cols = st.columns(len(result['shops_comparison']))
                    for i, (shop_name, stats_data) in enumerate(result['shops_comparison'].items()):
                        with cols[i]:
                            st.markdown(f"#### {shop_name}")
                            st.metric("å¹³å‡", f"{stats_data['mean']:,.0f}")
                            st.metric("æ¨™æº–åå·®", f"{stats_data['std']:,.0f}")
                            st.metric("ã‚µãƒ³ãƒ—ãƒ«æ•°", stats_data['n_samples'])

                    if result['statistical_test']:
                        st.markdown("---")
                        test = result['statistical_test']
                        col1, col2 = st.columns(2)
                        with col1:
                            st.markdown("**tæ¤œå®š**")
                            st.write(f"çµ±è¨ˆé‡: {test['t_test']['statistic']:.4f}")
                            st.write(f"på€¤: {test['t_test']['p_value']:.4f}")
                        with col2:
                            st.markdown("**Mann-Whitney Uæ¤œå®š**")
                            st.write(f"çµ±è¨ˆé‡: {test['mann_whitney_u']['statistic']:.4f}")
                            st.write(f"på€¤: {test['mann_whitney_u']['p_value']:.4f}")

                        if test['significant_difference']:
                            st.success("âœ… åº—èˆ—é–“ã«çµ±è¨ˆçš„ã«æœ‰æ„ãªå·®ãŒã‚ã‚Šã¾ã™")
                        else:
                            st.info("â„¹ï¸ æœ‰æ„ãªå·®ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            else:
                # ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ  + åˆ†å¸ƒåˆ¤å®š
                if selected_shop:
                    data = df[df["shop"] == selected_shop][selected_column].dropna().values
                else:
                    data = df[selected_column].dropna().values

                if len(data) > 0:
                    col1, col2 = st.columns([1, 1])

                    with col1:
                        fig = create_histogram_figure(data, selected_column, selected_shop)
                        st.pyplot(fig)
                        plt.close(fig)

                    with col2:
                        result = analyze_distribution(data, selected_column)
                        st.markdown(get_distribution_explanation(result))
                else:
                    st.warning("ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")

    # =========================
    # Tab 3: å…¨ä½“æ¦‚è¦
    # =========================
    with tab3:
        st.markdown("### ğŸ“Š å…¨ã‚«ãƒ©ãƒ çµ±è¨ˆæ¦‚è¦")

        summary_data = []
        for col in available_cols:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                summary_data.append({
                    "ã‚«ãƒ©ãƒ å": col,
                    "æ—¥æœ¬èªå": COLUMN_DEFINITIONS.get(col, col),
                    "ã‚µãƒ³ãƒ—ãƒ«æ•°": len(col_data),
                    "å¹³å‡": f"{col_data.mean():,.2f}",
                    "æ¨™æº–åå·®": f"{col_data.std():,.2f}",
                    "æœ€å°": f"{col_data.min():,.2f}",
                    "æœ€å¤§": f"{col_data.max():,.2f}",
                    "ä¸­å¤®å€¤": f"{col_data.median():,.2f}",
                })

        st.dataframe(pd.DataFrame(summary_data), use_container_width=True, height=400)

        # ç›¸é–¢è¡Œåˆ—
        if len(available_cols) >= 2:
            st.markdown("### ğŸ”— ç›¸é–¢è¡Œåˆ—ï¼ˆä¸Šä½10ã‚«ãƒ©ãƒ ï¼‰")
            corr_cols = available_cols[:10]
            corr_matrix = df[corr_cols].corr()

            fig, ax = plt.subplots(figsize=(10, 8))
            im = ax.imshow(corr_matrix, cmap='RdBu_r', vmin=-1, vmax=1)
            ax.set_xticks(range(len(corr_cols)))
            ax.set_yticks(range(len(corr_cols)))
            ax.set_xticklabels([COLUMN_DEFINITIONS.get(c, c)[:8] for c in corr_cols], rotation=45, ha='right')
            ax.set_yticklabels([COLUMN_DEFINITIONS.get(c, c)[:8] for c in corr_cols])
            plt.colorbar(im)
            plt.tight_layout()
            st.pyplot(fig)
            plt.close(fig)

    # ãƒ•ãƒƒã‚¿ãƒ¼
    st.markdown("---")
    st.markdown("""
    <div style="text-align: center; color: #888; font-size: 0.85rem;">
        Q-Storm EDA Distribution Analyzer v2.0 |
        Powered by Streamlit + SciPy |
        <b>API Key Not Required</b> âœ…
    </div>
    """, unsafe_allow_html=True)


if __name__ == "__main__":
    main()
